{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import cv2 \n",
    "import PIL \n",
    "import gc\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from tensorflow import set_random_seed\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.activations import softmax, relu\n",
    "from keras.optimizers import Adam, rmsprop, RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "np.random.seed(SEED) \n",
    "set_random_seed(SEED)\n",
    "\n",
    "# take in data\n",
    "df_train = pd.read_csv(\"../input/train.csv\") \n",
    "df_test  = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "\n",
    "# point data to file types\n",
    "df_train.id_code = df_train.id_code.apply(lambda x: x+\".png\")\n",
    "df_test.id_code  = df_test.id_code.apply(lambda x: x+\".png\")\n",
    "df_train['diagnosis'] = df_train['diagnosis'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['id_code', 'diagnosis'], dtype='object'),\n",
       " Index(['id_code'], dtype='object'),\n",
       " 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_DIM      = 300\n",
    "BATCH_SIZE   = 32\n",
    "CHANNEL_SIZE = 3\n",
    "NUM_EPOCHS   = 12\n",
    "# FREEZE_LAYERS = 2  # freeze the first this many layers for training\n",
    "NUM_CLASSES = df_train['diagnosis'].nunique()\n",
    "\n",
    "class_names = {\"0\":\"No DR\", \"1\":\"Mild\", \"2\":\"Moderate\", \"3\":\"Severe\", \"4\":\"Proliferative DR\"}\n",
    "\n",
    "df_train.columns, df_test.columns, NUM_CLASSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['diagnosis'].value_counts().plot(kind='bar');\n",
    "plt.title('Samples Per Class');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "def create_resnet(img_dim, CHANNELS, n_class):\n",
    "    input_tensor=Input(shape=(img_dim, img_dim, CHANNELS))\n",
    "  \n",
    "    base_model = ResNet50(weights=None, include_top=False, input_tensor=input_tensor)\n",
    "    base_model.load_weights('../resnet50/jam.h5')\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation=relu)(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = Dense(512, activation=relu)(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Dense(n_class, activation='softmax', name=\"Output_Layer\")(x)\n",
    "    model_resnet = Model(input_tensor, output_layer)\n",
    "    \n",
    "    return model_resnet\n",
    "\n",
    "model_resnet=create_resnet(IMG_DIM, CHANNEL_SIZE, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: don't think this is thinking of frozen layers\n",
    "for layers in model_resnet.layers:\n",
    "    layers.trainable = True\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = rmsprop(lr=lr, decay=0.1)\n",
    "model_resnet.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy,  metrics=['accuracy'])\n",
    "# model_resnet.summary()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 106 layers into a model with 110 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-679cc6f90cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_resnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../resnet50/base.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 106 layers into a model with 110 layers."
     ]
    }
   ],
   "source": [
    "# Run on test data\n",
    "model_resnet.load_weights('../resnet50/jam.h5')\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "test_df.head()\n",
    "test_df['id_code'] = test_df['id_code'].apply(lambda x:  x + '.png')\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(  dataframe=test_df, \n",
    "                                                    directory=\"../input/test_images/\", \n",
    "                                                    x_col='id_code', \n",
    "                                                    y_col=None,\n",
    "                                                    shuffle=False,\n",
    "                                                    class_mode=None,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    target_size=(IMG_DIM,IMG_DIM)\n",
    "                                                 )\n",
    "\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "\n",
    "\n",
    "predict_output=model_resnet.predict_generator(test_generator,\n",
    "                                                steps=STEP_SIZE_TEST,\n",
    "                                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in submission file\n",
    "y_test= np.argmax(predict_output,axis=1)\n",
    "print(y_test)\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "print(test_df)\n",
    "print(y_test.shape)\n",
    "test_df['diagnosis'] = y_test\n",
    "test_df.to_csv('submission.csv',index=False)\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2 \n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import DenseNet121\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.activations import softmax, relu, sigmoid\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Global constants\n",
    "IMG_DIM      = 256\n",
    "BATCH_SIZE   = 32\n",
    "CHANNEL_SIZE = 3\n",
    "NUM_CLASSES  = 5\n",
    "\n",
    "class_names = {\"0\":\"No DR\", \"1\":\"Mild\", \"2\":\"Moderate\", \"3\":\"Severe\", \"4\":\"Proliferative DR\"}\n",
    "\n",
    "# data frame of current competition\n",
    "df_2019 = pd.read_csv(f\"../aptos2019/train.csv\") \n",
    "df_2019.id_code = df_2019.id_code.apply(lambda x: x + \".png\")\n",
    "df_2019.id_code = df_2019.id_code.apply(lambda x: \"modified_\" + x) \n",
    "train_2019, valid_2019 = train_test_split(df_2019, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 2019 data frame \n",
    "df_2015 = pd.read_csv(f\"../aptos2015/trainLabels.csv\") \n",
    "df_2015.image   = df_2015.image.apply(lambda   x: x + \".jpeg\")\n",
    "df_2015[\"id_code\"]   = df_2015.image\n",
    "df_2015[\"diagnosis\"] = df_2015.level\n",
    "train_2015, valid_2015 = train_test_split(df_2015, test_size=0.2, shuffle=False)\n",
    "\n",
    "# valid_2019['diagnosis'].value_counts().plot(kind='bar')\n",
    "# plt.title('Samples Per Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "In this kernel, we are using multilabel data. Instead of predicting a single label, we will change our target to be a multilabel problem; i.e., if the target is a certain class, then it encompasses all the classes before it. E.g. encoding a class 4 retinopathy would usually be `[0, 0, 0, 1]`, but in our case we will predict `[1, 1, 1, 1]`. \n",
    "\n",
    "The idea is that if an eye has severe diabetic retinopathy, that also means that it has moderate and severe diabetic retinopathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370\n",
      "                        id_code diagnosis           labels\n",
      "3166  modified_dbb2c63f6f08.png         3     [0, 1, 2, 3]\n",
      "2938  modified_cbd0870aa933.png         4  [0, 1, 2, 3, 4]\n",
      "2970  modified_ce207b69ff37.png         3     [0, 1, 2, 3]\n",
      "3181  modified_dd285d9e97fe.png         0              [0]\n",
      "3494  modified_f36cb007a1ef.png         0              [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def label_convert(y_val):\n",
    "    y_val = y_val.astype(int).sum(axis=1) - 1\n",
    "    #y_val= np.argmax(y_val, axis=1)\n",
    "    return y_val\n",
    "\n",
    "def get_train_valid_df(year=\"2019\", even_distrib=True):\n",
    "    \n",
    "    # shuffle data so each time different samples are dropped\n",
    "    if (year == \"2019\"):\n",
    "        train = train_2019.sample(frac=1)\n",
    "        valid = valid_2019.sample(frac=1)\n",
    "    elif (year == \"2015\"):\n",
    "        train = train_2015.sample(frac=1)\n",
    "        valid = valid_2015.sample(frac=1)\n",
    "    \n",
    "    # remap from classes to smoothed version of the classes\n",
    "    train[\"labels\"] = train.diagnosis.apply(lambda x: [i for i in range(x + 1)])\n",
    "    valid[\"labels\"] = valid.diagnosis.apply(lambda x: [i for i in range(x + 1)])\n",
    "\n",
    "    train.diagnosis = train.diagnosis.astype('str')\n",
    "    valid.diagnosis = valid.diagnosis.astype('str')\n",
    "\n",
    "    # drop classes \n",
    "    if even_distrib:\n",
    "        min_train = min(train['diagnosis'].value_counts())\n",
    "        min_valid = min(valid['diagnosis'].value_counts())\n",
    "\n",
    "        for diagnosis in range(5):\n",
    "            indexes_valid = valid[valid['diagnosis'] == str(diagnosis)].index\n",
    "            indexes_train = train[train['diagnosis'] == str(diagnosis)].index\n",
    "            \n",
    "            frac_drop_train = indexes_train.size * (1 - min_train/indexes_train.size)\n",
    "            frac_drop_valid = indexes_valid.size * (1 - min_valid/indexes_valid.size)\n",
    "            \n",
    "            train.drop(indexes_train[:int(frac_drop_train)], inplace=True)\n",
    "            valid.drop(indexes_valid[:int(frac_drop_valid)], inplace=True)\n",
    "\n",
    "    # shuffle it for even distribution later, and take a smaller batch if the train size is larger than 10000\n",
    "    frac = 1\n",
    "    print(train.size)\n",
    "    if train.size > 10000:\n",
    "        frac = 10000/train.size\n",
    "    train = train.sample(frac=frac)\n",
    "    valid = valid.sample(frac=frac)\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "# plot example\n",
    "_, df_to_plot = get_train_valid_df(year=\"2019\")\n",
    "df_to_plot['diagnosis'].value_counts().plot(kind='bar')\n",
    "plt.title('Samples Per Class')\n",
    "print(df_to_plot.head(5))\n",
    "\n",
    "# df_to_plot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some data\n",
    "# df_example, _ = get_train_valid_df(year=\"2019\")\n",
    "\n",
    "# # Display some random images from Data Set with class categories.\n",
    "# figure=plt.figure(figsize=(22,20))\n",
    "# for target_class in (df_example['diagnosis'].unique()):\n",
    "#     for i, (idx, row ) in enumerate(df_example.loc[df_example.diagnosis == target_class]\n",
    "#                                     .sample(4)\n",
    "#                                     .iterrows()):\n",
    "#         # open the file\n",
    "#         imagefile = f\"../aptos2019/train_images/{row['id_code']}\" \n",
    "#         img = cv2.imread(imagefile)\n",
    "        \n",
    "#         # original version\n",
    "#         rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         ax = figure.add_subplot(5,4, int(target_class)*4+i+1)\n",
    "#         plt.imshow(rgb)\n",
    "#         ax.set_title(class_names[target_class])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data generators, which put their data into a random crop generator, which is then fed into\n",
    "# the network during training\n",
    "\n",
    "def dataGenerator(jitter=0.1):\n",
    "    datagen = image.ImageDataGenerator(rescale=1./255,\n",
    "                                       horizontal_flip=True, \n",
    "                                       vertical_flip=True,\n",
    "                                       rotation_range=int(800*jitter),\n",
    "                                       brightness_range=[1-jitter, 1+jitter],\n",
    "                                       channel_shift_range=int(20*jitter),\n",
    "                                       zoom_range=[(1-jitter), (1+jitter/2)],\n",
    "                                       fill_mode=\"reflect\",\n",
    "                                      )\n",
    "    return datagen\n",
    "\n",
    "def datagen_with_flow(datagen, dataframe, directory):\n",
    "    return datagen.flow_from_dataframe(dataframe=dataframe, directory=directory,\n",
    "                                       x_col=\"id_code\", \n",
    "                                       y_col='labels', \n",
    "                                       class_mode=\"categorical\", \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       target_size=(IMG_DIM, IMG_DIM),\n",
    "                                       shuffle=False,\n",
    "                                      )\n",
    "\n",
    "def generator(jitter=0.1, year=\"2019\", even_distrib=True):\n",
    "    \n",
    "    train, valid = get_train_valid_df(year=year, even_distrib=even_distrib) \n",
    "    datagen = dataGenerator(jitter)\n",
    "    \n",
    "    train_gen = datagen_with_flow(datagen, train, f\"../aptos{year}/train_images/\")\n",
    "    valid_gen = datagen_with_flow(datagen, valid, f\"../aptos{year}/train_images/\")\n",
    "    \n",
    "    return train_gen, valid_gen\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample_gen, valid_sample_gen = generator(jitter=0.05)\n",
    "\n",
    "# # Display some data generation\n",
    "# figure=plt.figure(figsize=(22,20))\n",
    "# for batch in valid_sample_gen:\n",
    "#     for j in range(16):\n",
    "#         ax = figure.add_subplot(4,4, j+1)\n",
    "#         batch[0][j] = np.clip(batch[0][j], 0, 1)\n",
    "#         plt.imshow(batch[0][j])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        numBatches = 15\n",
    "        y_pred = []\n",
    "        y_val = []\n",
    "        for x, y in self.generator:\n",
    "            predictions = model.predict(x) \n",
    "            predictions = predictions > 0.5\n",
    "            y_pred.extend(label_convert(predictions))\n",
    "            y_val.extend(label_convert(y))\n",
    "            \n",
    "            numBatches -= 1\n",
    "            if numBatches <= 0:\n",
    "                break\n",
    "            \n",
    "        val_kappa = cohen_kappa_score(y_val, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(val_kappa)\n",
    "        \n",
    "        print(val_kappa)\n",
    "        print(confusion_matrix(y_val, y_pred))\n",
    "            \n",
    "        if val_kappa == max(self.val_kappas) and val_kappa > 0.84:\n",
    "            gc.collect()\n",
    "            print(\"Max of this run, saving model.\")\n",
    "            model.save(f\"dense-multi-{val_kappa:.4f}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_dim, channels, n_class):\n",
    "    \n",
    "    input_tensor = Input(shape=(img_dim, img_dim, channels))\n",
    "    base_model   = DenseNet121(weights=None, include_top=False, input_tensor=input_tensor)\n",
    "#     base_model.load_weights('../DenseNet-BC-121-32-no-top.h5')\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512, activation=relu)(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Dense(n_class, activation='sigmoid', name=\"Output_Layer\")(x)\n",
    "    model = Model(input_tensor, output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(IMG_DIM, CHANNEL_SIZE, NUM_CLASSES)\n",
    "model.load_weights(\"dense-multi-0.8865.h5\") \n",
    "model.save_weights(\"trial_save.h5\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ----------------------------------- 0.2 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    2019 False -   -   -   -   -   -   -   -   -\n",
      "8787\n",
      "Found 2929 validated image filenames belonging to 5 classes.\n",
      "Found 733 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "91/91 [==============================] - 181s 2s/step - loss: 0.0740 - acc: 0.5934 - val_loss: 0.0456 - val_acc: 0.5838\n",
      "0.8865201439764823\n",
      "[[250   2   0   0   1]\n",
      " [  4  17  19   1   1]\n",
      " [  3   5  88  14   9]\n",
      " [  0   0  13   9   6]\n",
      " [  0   0  10   5  20]]\n",
      "Max of this run, saving model.\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 107s 1s/step - loss: 0.0551 - acc: 0.5574 - val_loss: 0.0427 - val_acc: 0.5749\n",
      "0.8745548397423072\n",
      "[[240   2   1   0   0]\n",
      " [  7  18  20   1   2]\n",
      " [  2  11  92   5   6]\n",
      " [  0   1  16   3   8]\n",
      " [  0   2  13   8  22]]\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0488 - acc: 0.5436 - val_loss: 0.0356 - val_acc: 0.5802\n",
      "0.8911088297606763\n",
      "[[236   3   0   0   0]\n",
      " [  7  15  18   0   0]\n",
      " [  3   9 110   4   1]\n",
      " [  0   0  15   4   3]\n",
      " [  0   1  19  11  18]]\n",
      "Max of this run, saving model.\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 106s 1s/step - loss: 0.0459 - acc: 0.5388 - val_loss: 0.0415 - val_acc: 0.5866\n",
      "0.8590413294719463\n",
      "[[245   3   0   1   0]\n",
      " [  6  18  16   1   1]\n",
      " [  3  10  99   5   2]\n",
      " [  0   0  18   3   4]\n",
      " [  0   1  25   4  15]]\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0423 - acc: 0.5302 - val_loss: 0.0350 - val_acc: 0.5516\n",
      "0.8633984059266555\n",
      "[[245   5   0   0   0]\n",
      " [  4  20  15   0   0]\n",
      " [  2  12  98   2   2]\n",
      " [  0   1  12   3   4]\n",
      " [  0   2  30   2  18]]\n",
      "           -   -   -   -   -   -   -   -    2015 False -   -   -   -   -   -   -   -   -\n",
      "140500\n",
      "Found 2000 validated image filenames belonging to 5 classes.\n",
      "Found 500 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "62/62 [==============================] - 75s 1s/step - loss: 0.1260 - acc: 0.7641 - val_loss: 0.0681 - val_acc: 0.9667\n",
      "0.2537009477559333\n",
      "[[351   5   1   0   0]\n",
      " [ 38   1   0   0   0]\n",
      " [ 47  16   7   0   0]\n",
      " [  1   1   3   0   0]\n",
      " [  7   2   0   0   0]]\n",
      "Epoch 2/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1043 - acc: 0.8342 - val_loss: 0.0706 - val_acc: 0.9539\n",
      "0.3235086729657314\n",
      "[[352   1   0   0   0]\n",
      " [ 35   0   0   0   2]\n",
      " [ 52   6   6   0   0]\n",
      " [  2   0   2   1   0]\n",
      " [  4   0   4   1   0]]\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.0914 - acc: 0.8690 - val_loss: 0.0807 - val_acc: 0.9124\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "0.5831106360235168\n",
      "[[306  25  12   1   2]\n",
      " [ 33   4   0   0   0]\n",
      " [ 28  11  18   4   9]\n",
      " [  2   0   2   1   1]\n",
      " [  0   2   0   1   6]]\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.0856 - acc: 0.8780 - val_loss: 0.0720 - val_acc: 0.9295\n",
      "0.5252755790802701\n",
      "[[297  29  14   0   0]\n",
      " [ 35   2   1   0   0]\n",
      " [ 34  15  15   2   9]\n",
      " [  2   1   0   1   1]\n",
      " [  1   2   2   1   4]]\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.0857 - acc: 0.8715 - val_loss: 0.0688 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
      "0.4791635172587554\n",
      "[[292  24  13   1   1]\n",
      " [ 29   5   1   0   0]\n",
      " [ 33  14  15   3   7]\n",
      " [  2   1   3   0   2]\n",
      " [  2   4   1   1   2]]\n",
      "           -   -   -   -   -   -   -   -    2019 False -   -   -   -   -   -   -   -   -\n",
      "8787\n",
      "Found 2929 validated image filenames belonging to 5 classes.\n",
      "Found 733 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.0999 - acc: 0.7857 - val_loss: 0.0817 - val_acc: 0.7216\n",
      "0.7515625\n",
      "[[246   1   0   0   0]\n",
      " [ 14   5  11   0   4]\n",
      " [ 19  13  59   3  27]\n",
      " [  1   1  12   5   9]\n",
      " [  2   1  27   5  12]]\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0984 - acc: 0.7842 - val_loss: 0.0775 - val_acc: 0.7518\n",
      "0.6541999597126167\n",
      "[[259   0   0   0   0]\n",
      " [ 21   6   8   0   2]\n",
      " [ 23  20  61   3  15]\n",
      " [  1   4  14   2   4]\n",
      " [  8   5  19   1   4]]\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0952 - acc: 0.7804 - val_loss: 0.0847 - val_acc: 0.7593\n",
      "0.6231168427985887\n",
      "[[243   2   0   0   0]\n",
      " [ 24   8   8   1   1]\n",
      " [ 31  22  57   3   8]\n",
      " [  3   1  13   2   5]\n",
      " [  8   6  26   2   3]]\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0948 - acc: 0.7777 - val_loss: 0.0810 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-08.\n",
      "0.6774631444628458\n",
      "[[258   0   0   0   0]\n",
      " [ 21  10   5   0   2]\n",
      " [ 28  19  53   5  12]\n",
      " [  1   3  15   1   6]\n",
      " [  6   3  25   2   5]]\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0901 - acc: 0.7701 - val_loss: 0.0823 - val_acc: 0.7736\n",
      "0.6237942122186495\n",
      "[[244   1   0   0   0]\n",
      " [ 25   7   9   0   1]\n",
      " [ 32  21  54   5  13]\n",
      " [  2   1  14   3   2]\n",
      " [ 12   5  15   0  11]]\n",
      "           -   -   -   -   -   -   -   -    2019 True -   -   -   -   -   -   -   -   -\n",
      "2370\n",
      "Found 790 validated image filenames belonging to 5 classes.\n",
      "Found 175 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 32s 1s/step - loss: 0.1971 - acc: 0.7604 - val_loss: 0.1626 - val_acc: 0.6125\n",
      "0.4717804580880979\n",
      "[[89  0  0  0  0]\n",
      " [51 16 18  0  2]\n",
      " [28  9 36  5  6]\n",
      " [11  7 50  3 16]\n",
      " [16 11 42  2 11]]\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2069 - acc: 0.7627 - val_loss: 0.1635 - val_acc: 0.6434\n",
      "0.4548762412750742\n",
      "[[88  0  0  0  0]\n",
      " [48 17 18  0  3]\n",
      " [32 13 40  1  4]\n",
      " [ 9  5 56  5 14]\n",
      " [22 11 42  3 15]]\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2035 - acc: 0.7645 - val_loss: 0.1671 - val_acc: 0.6294\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "0.4424940643828673\n",
      "[[89  0  0  0  0]\n",
      " [56 15 15  0  1]\n",
      " [29 17 31  1  6]\n",
      " [12  9 49  7 10]\n",
      " [22  8 39  2 11]]\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2081 - acc: 0.7634 - val_loss: 0.1677 - val_acc: 0.6993\n",
      "0.4029205337937912\n",
      "[[84  0  0  0  0]\n",
      " [52 13 16  0  2]\n",
      " [30 16 35  0  5]\n",
      " [12  9 47  5 11]\n",
      " [24 13 44  1 10]]\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2043 - acc: 0.7556 - val_loss: 0.1707 - val_acc: 0.6500\n",
      "0.4024821614398162\n",
      "[[86  0  0  0  0]\n",
      " [56 13 16  0  3]\n",
      " [37 17 31  1  5]\n",
      " [ 9 11 51  1 16]\n",
      " [28 12 38  2 13]]\n",
      "           -   -   -   -   -   -   -   -    2015 True -   -   -   -   -   -   -   -   -\n",
      "13930\n",
      "Found 2000 validated image filenames belonging to 5 classes.\n",
      "Found 543 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "62/62 [==============================] - 75s 1s/step - loss: 0.2179 - acc: 0.8070 - val_loss: 0.2817 - val_acc: 0.9199\n",
      "0.14628801790234325\n",
      "[[82  0  0  0  0]\n",
      " [95  0  0  0  0]\n",
      " [89  3  2  0  0]\n",
      " [56 12 23  3  6]\n",
      " [70 17 16  3  2]]\n",
      "Epoch 2/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.2172 - acc: 0.8120 - val_loss: 0.2491 - val_acc: 0.8826\n",
      "0.20147069641119975\n",
      "[[88  0  0  0  0]\n",
      " [96  0  0  0  0]\n",
      " [85 11  3  0  0]\n",
      " [47 14 34  4  3]\n",
      " [52 14 24  0  4]]\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.2189 - acc: 0.7964 - val_loss: 0.2480 - val_acc: 0.8689\n",
      "0.2293027791321306\n",
      "[[ 94   1   0   0   0]\n",
      " [101   0   0   0   0]\n",
      " [ 80   8   9   0   1]\n",
      " [ 33  21  36   2   4]\n",
      " [ 50  12  21   1   5]]\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.2211 - acc: 0.8100 - val_loss: 0.2475 - val_acc: 0.8748\n",
      "0.22162246181237544\n",
      "[[85  1  0  0  0]\n",
      " [90  0  0  0  0]\n",
      " [85 10  8  0  0]\n",
      " [40 14 36  6  7]\n",
      " [48 21 24  2  2]]\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.2169 - acc: 0.7959 - val_loss: 0.2294 - val_acc: 0.8434\n",
      "0.24337176313566156\n",
      "[[84  0  0  0  0]\n",
      " [93  0  0  0  0]\n",
      " [83 11  7  0  0]\n",
      " [39 10 37  4  8]\n",
      " [47 19 31  3  3]]\n",
      "           -   -   -   -   -   -   -   -    2019 True -   -   -   -   -   -   -   -   -\n",
      "2370\n",
      "Found 790 validated image filenames belonging to 5 classes.\n",
      "Found 175 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 32s 1s/step - loss: 0.1920 - acc: 0.7786 - val_loss: 0.1566 - val_acc: 0.5375\n",
      "0.44881434355118566\n",
      "[[90  0  0  0  0]\n",
      " [37 20 22  2  5]\n",
      " [20 12 53  2  6]\n",
      " [ 1  8 63  6 10]\n",
      " [18 14 48  3  6]]\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2048 - acc: 0.7627 - val_loss: 0.1604 - val_acc: 0.5664\n",
      "0.4352797260226958\n",
      "[[88  0  0  0  0]\n",
      " [45 21 21  1  2]\n",
      " [25 12 42  2  8]\n",
      " [ 3  5 61  6 13]\n",
      " [25 11 44  3  8]]\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.2016 - acc: 0.7718 - val_loss: 0.1583 - val_acc: 0.5594\n",
      "0.43926877953718824\n",
      "[[88  0  0  0  0]\n",
      " [39 17 20  1  2]\n",
      " [28  9 41  6  5]\n",
      " [ 5 11 51  6 15]\n",
      " [20 14 41  1  9]]\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1998 - acc: 0.7699 - val_loss: 0.1557 - val_acc: 0.5714\n",
      "0.39665045357947526\n",
      "[[87  0  0  0  0]\n",
      " [55 18 22  0  1]\n",
      " [25 12 40  3  6]\n",
      " [ 8  7 53  3 16]\n",
      " [31 11 38  1  9]]\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1998 - acc: 0.7692 - val_loss: 0.1726 - val_acc: 0.6188\n",
      "0.41936155808988185\n",
      "[[90  0  0  0  0]\n",
      " [53 13 23  1  0]\n",
      " [29  8 43  2  5]\n",
      " [14  6 54  3 13]\n",
      " [25 10 41  3 10]]\n",
      "           ----------------------------------- 0.05 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    2019 False -   -   -   -   -   -   -   -   -\n",
      "8787\n",
      "Found 2929 validated image filenames belonging to 5 classes.\n",
      "Found 733 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "91/91 [==============================] - 183s 2s/step - loss: 0.0573 - acc: 0.6435 - val_loss: 0.0500 - val_acc: 0.7642\n",
      "0.8066679741125711\n",
      "[[257   0   1   0   0]\n",
      " [ 17  17  10   0   0]\n",
      " [ 10  10  89   3   4]\n",
      " [  0   1   8   3   5]\n",
      " [  2   4  22   6   8]]\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0449 - acc: 0.5861 - val_loss: 0.0439 - val_acc: 0.5877\n",
      "0.880076764049386\n",
      "[[245   6   1   0   0]\n",
      " [  4  15  22   0   2]\n",
      " [  0  11 108   4   6]\n",
      " [  0   1   9   3   5]\n",
      " [  0   1  14   6  17]]\n",
      "Max of this run, saving model.\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0411 - acc: 0.5748 - val_loss: 0.0349 - val_acc: 0.6046\n",
      "0.8929242031706796\n",
      "[[257   4   0   0   0]\n",
      " [  2  14  19   0   0]\n",
      " [  1  14  88   4   7]\n",
      " [  0   0  15   6   6]\n",
      " [  0   2  12   8  18]]\n",
      "Max of this run, saving model.\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 106s 1s/step - loss: 0.0388 - acc: 0.5665 - val_loss: 0.0433 - val_acc: 0.5384\n",
      "0.8871494163854622\n",
      "[[275   6   0   0   0]\n",
      " [  2  19  13   0   0]\n",
      " [  1  15  97   0   0]\n",
      " [  0   0  10   2   3]\n",
      " [  0   2  20   2  13]]\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 106s 1s/step - loss: 0.0355 - acc: 0.5572 - val_loss: 0.0443 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "0.8637095828013389\n",
      "[[239   5   1   0   0]\n",
      " [  5  20  11   0   0]\n",
      " [  4  19  93   6   0]\n",
      " [  0   1  20   5   3]\n",
      " [  0   3  20   7  15]]\n",
      "           -   -   -   -   -   -   -   -    2015 False -   -   -   -   -   -   -   -   -\n",
      "140500\n",
      "Found 2000 validated image filenames belonging to 5 classes.\n",
      "Found 500 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "62/62 [==============================] - 75s 1s/step - loss: 0.1294 - acc: 0.7601 - val_loss: 0.0655 - val_acc: 0.9042\n",
      "0.6210703592814371\n",
      "[[315  32  11   0   1]\n",
      " [ 27   3   1   0   0]\n",
      " [ 18  21  27   1   5]\n",
      " [  1   3  11   0   0]\n",
      " [  0   0   1   0   2]]\n",
      "Epoch 2/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1161 - acc: 0.7979 - val_loss: 0.0758 - val_acc: 0.8838\n",
      "0.5401413139743101\n",
      "[[281  53  20   0   3]\n",
      " [ 28   7   0   0   0]\n",
      " [ 13  18  30   2   1]\n",
      " [  0   2   6   0   0]\n",
      " [  0   0   1   1   2]]\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1090 - acc: 0.8160 - val_loss: 0.0757 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
      "0.5959071472205253\n",
      "[[266  67  22   1   0]\n",
      " [ 21   6   0   0   0]\n",
      " [ 10  21  30   3   3]\n",
      " [  0   4   8   1   0]\n",
      " [  0   0   2   1   2]]\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1027 - acc: 0.8322 - val_loss: 0.0718 - val_acc: 0.8697\n",
      "0.563313741537088\n",
      "[[270  67  22   0   1]\n",
      " [ 16   9   0   0   0]\n",
      " [ 14  17  33   1   1]\n",
      " [  0   2   8   1   0]\n",
      " [  0   1   2   3   0]]\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1026 - acc: 0.8206 - val_loss: 0.0799 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-08.\n",
      "0.5845985778011552\n",
      "[[254  66  24   0   4]\n",
      " [ 15   8   2   0   0]\n",
      " [ 10  19  42   0   2]\n",
      " [  0   1   9   3   2]\n",
      " [  0   1   4   1   1]]\n",
      "           -   -   -   -   -   -   -   -    2019 False -   -   -   -   -   -   -   -   -\n",
      "8787\n",
      "Found 2929 validated image filenames belonging to 5 classes.\n",
      "Found 733 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.0422 - acc: 0.6346 - val_loss: 0.0444 - val_acc: 0.5938\n",
      "0.8470544395722481\n",
      "[[239   4   2   0   0]\n",
      " [  4  24  15   0   1]\n",
      " [  2  11 102   2   3]\n",
      " [  0   0  17   5   6]\n",
      " [  0   1  29   1   9]]\n",
      "Max of this run, saving model.\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0409 - acc: 0.6359 - val_loss: 0.0418 - val_acc: 0.6348\n",
      "0.8472403027545131\n",
      "[[254   2   0   0   0]\n",
      " [  5  19  13   0   0]\n",
      " [  2  20  94   2   0]\n",
      " [  0   1  19   1   4]\n",
      " [  0   4  26   4  10]]\n",
      "Max of this run, saving model.\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 106s 1s/step - loss: 0.0405 - acc: 0.6424 - val_loss: 0.0465 - val_acc: 0.6160\n",
      "0.8461944334520511\n",
      "[[244   1   1   0   0]\n",
      " [  4  24  13   0   0]\n",
      " [  4  27  91   1   1]\n",
      " [  0   0  17   3   1]\n",
      " [  0   2  28   2  13]]\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 105s 1s/step - loss: 0.0404 - acc: 0.6349 - val_loss: 0.0399 - val_acc: 0.6406\n",
      "0.8347537921310475\n",
      "[[245   4   0   0   0]\n",
      " [  5  25  12   0   0]\n",
      " [  5  25  97   2   3]\n",
      " [  0   0  15   2   0]\n",
      " [  2   2  21   1  14]]\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 106s 1s/step - loss: 0.0419 - acc: 0.6390 - val_loss: 0.0461 - val_acc: 0.6461\n",
      "0.8822433838319653\n",
      "[[255   1   0   0   0]\n",
      " [  4  20   9   0   0]\n",
      " [  2  30  94   1   3]\n",
      " [  0   1  14   1   2]\n",
      " [  0   0  20   4  16]]\n",
      "Max of this run, saving model.\n",
      "           -   -   -   -   -   -   -   -    2019 True -   -   -   -   -   -   -   -   -\n",
      "2370\n",
      "Found 790 validated image filenames belonging to 5 classes.\n",
      "Found 175 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 32s 1s/step - loss: 0.1321 - acc: 0.6641 - val_loss: 0.1023 - val_acc: 0.4437\n",
      "0.6573625351647044\n",
      "[[88  0  0  0  0]\n",
      " [12 46 27  0  0]\n",
      " [ 2 21 60  1  0]\n",
      " [ 0  3 73  4  5]\n",
      " [ 5 12 42  2 26]]\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1275 - acc: 0.6627 - val_loss: 0.1078 - val_acc: 0.4965\n",
      "0.6189360130991749\n",
      "[[88  0  0  0  0]\n",
      " [15 44 26  0  0]\n",
      " [ 3 24 55  0  2]\n",
      " [ 0  4 74  3  4]\n",
      " [ 6 17 37  5 22]]\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1309 - acc: 0.6627 - val_loss: 0.1073 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "0.6452162905010094\n",
      "[[89  0  0  0  0]\n",
      " [26 41 22  0  0]\n",
      " [ 3 30 59  0  0]\n",
      " [ 0  4 75  4  8]\n",
      " [ 5 13 42  4 21]]\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1304 - acc: 0.6601 - val_loss: 0.1093 - val_acc: 0.5312\n",
      "0.6324527214570022\n",
      "[[86  0  0  0  0]\n",
      " [22 46 18  0  0]\n",
      " [ 4 25 58  0  1]\n",
      " [ 0  4 72  2  7]\n",
      " [ 7  9 46  2 20]]\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1278 - acc: 0.6692 - val_loss: 0.1240 - val_acc: 0.5804\n",
      "0.6080466439296379\n",
      "[[89  0  0  0  0]\n",
      " [21 47 19  0  0]\n",
      " [10 28 57  0  0]\n",
      " [ 0  7 73  1  6]\n",
      " [ 6 14 45  4 19]]\n",
      "           -   -   -   -   -   -   -   -    2015 True -   -   -   -   -   -   -   -   -\n",
      "13930\n",
      "Found 2000 validated image filenames belonging to 5 classes.\n",
      "Found 543 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.1855 - acc: 0.7465 - val_loss: 0.2394 - val_acc: 0.8340\n",
      "0.32647484327278564\n",
      "[[ 84   0   0   0   0]\n",
      " [103   0   0   0   0]\n",
      " [ 75  10   5   0   0]\n",
      " [ 34  19  44   0   0]\n",
      " [ 29  28  29   5  15]]\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 71s 1s/step - loss: 0.1847 - acc: 0.7480 - val_loss: 0.2144 - val_acc: 0.8020\n",
      "0.37550947267178525\n",
      "[[ 83   1   0   0   0]\n",
      " [112   0   0   0   0]\n",
      " [ 76  12   7   0   0]\n",
      " [ 24  19  50   1   0]\n",
      " [ 20  20  37   7  10]]\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1864 - acc: 0.7399 - val_loss: 0.2032 - val_acc: 0.7730\n",
      "0.4360633856944949\n",
      "[[97  2  0  0  0]\n",
      " [95  2  0  0  0]\n",
      " [75 14  5  0  0]\n",
      " [22 16 57  0  0]\n",
      " [18  9 42 12 13]]\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1881 - acc: 0.7419 - val_loss: 0.2029 - val_acc: 0.7691\n",
      "0.4462254718160231\n",
      "[[97  5  1  0  0]\n",
      " [83  3  0  0  0]\n",
      " [60 17  8  0  0]\n",
      " [27 20 61  1  0]\n",
      " [14 13 44  8 17]]\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 71s 1s/step - loss: 0.1869 - acc: 0.7404 - val_loss: 0.2060 - val_acc: 0.7632\n",
      "0.46615658827109496\n",
      "[[90  5  0  0  0]\n",
      " [75  4  0  0  0]\n",
      " [64 23  5  0  0]\n",
      " [23 20 63  1  0]\n",
      " [ 7 20 55  7 18]]\n",
      "           -   -   -   -   -   -   -   -    2019 True -   -   -   -   -   -   -   -   -\n",
      "2370\n",
      "Found 790 validated image filenames belonging to 5 classes.\n",
      "Found 175 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 31s 1s/step - loss: 0.1313 - acc: 0.6706 - val_loss: 0.1041 - val_acc: 0.4062\n",
      "0.6850849050820442\n",
      "[[86  0  0  0  0]\n",
      " [16 47 20  0  0]\n",
      " [ 2 17 64  0  0]\n",
      " [ 0  4 76  2  4]\n",
      " [ 0  8 58  6 19]]\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1222 - acc: 0.6549 - val_loss: 0.1113 - val_acc: 0.4406\n",
      "0.6642188658265382\n",
      "[[93  0  0  0  0]\n",
      " [26 43 23  0  0]\n",
      " [ 3 17 66  0  0]\n",
      " [ 0  2 79  1  6]\n",
      " [ 1 13 54  0 19]]\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1276 - acc: 0.6580 - val_loss: 0.1009 - val_acc: 0.4126\n",
      "0.637263866461742\n",
      "[[86  0  0  0  0]\n",
      " [28 39 16  0  0]\n",
      " [ 6 23 53  1  0]\n",
      " [ 0  5 75  2  4]\n",
      " [ 1 16 53  2 19]]\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1270 - acc: 0.6470 - val_loss: 0.1140 - val_acc: 0.4965\n",
      "0.6417954197041245\n",
      "[[89  0  0  0  0]\n",
      " [23 48 21  0  0]\n",
      " [ 9 17 65  1  0]\n",
      " [ 0  5 76  2  6]\n",
      " [ 3 10 52  2 17]]\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 27s 1s/step - loss: 0.1259 - acc: 0.6694 - val_loss: 0.1089 - val_acc: 0.4813\n",
      "0.6213648350854377\n",
      "[[90  0  0  0  0]\n",
      " [28 42 18  0  0]\n",
      " [10 19 62  0  0]\n",
      " [ 0  6 73  4  4]\n",
      " [ 5 15 47  2 21]]\n"
     ]
    }
   ],
   "source": [
    " for jitter in [0.2, 0.05]:\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss=keras.losses.mean_squared_error,  metrics=['accuracy'])\n",
    "    \n",
    "    print(\"           -----------------------------------\", jitter, \"-----------------------------------\")\n",
    "    \n",
    "    for even_distrib in [False, True]:\n",
    "        \n",
    "        for year in [\"2019\", \"2015\", \"2019\"]:\n",
    "\n",
    "            print(\"           -   -   -   -   -   -   -   -   \", year, even_distrib, \"-   -   -   -   -   -   -   -   -\")\n",
    "            # these need to be global for the kappa callback\n",
    "            train_generator, valid_generator = generator(jitter=0.1, year=year, even_distrib=even_distrib)\n",
    "\n",
    "            # Call backs during training            \n",
    "            kappa_callbacks = Metrics(valid_generator)\n",
    "            reduce_lr  = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, \n",
    "                                           min_lr=1e-8, mode='auto', verbose=1)\n",
    "\n",
    "            # train the model for 12 epochs\n",
    "            history = model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=train_generator.n  // train_generator.batch_size,\n",
    "                                          validation_data=valid_generator,\n",
    "                                          validation_steps=valid_generator.n // valid_generator.batch_size,\n",
    "                                          epochs=5,\n",
    "                                          callbacks=[reduce_lr, kappa_callbacks],\n",
    "                                          verbose=1\n",
    "                                         )\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"dense-multi-second-run.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

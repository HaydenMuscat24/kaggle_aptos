{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2 \n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.applications import DenseNet121\n",
    "from keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.activations import sigmoid\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Global constants\n",
    "IMG_DIM      = 224\n",
    "BATCH_SIZE   = 32\n",
    "NUM_CLASSES  = 4\n",
    "CHANNEL_SIZE = 3\n",
    "\n",
    "pre_process = \"ben_colour_\"\n",
    "\n",
    "# pre_process = \"ben_green_\"\n",
    "\n",
    "# pre_process = \"clahe_green_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "In this kernel, we are using multilabel data. Instead of predicting a single label, we will change our target to be a multilabel problem; i.e., if the target is a certain class, then it encompasses all the classes before it. E.g. encoding a class 4 retinopathy would usually be `[0, 0, 0, 1]`, but in our case we will predict `[1, 1, 1, 1]`. \n",
    "\n",
    "The idea is that if an eye has severe diabetic retinopathy, that also means that it has moderate and severe diabetic retinopathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id_code  diagnosis          original                    processed\n",
      "2929  cb28adab4e8a          0  cb28adab4e8a.png  ben_colour_cb28adab4e8a.png\n",
      "2930  cb2f3c5d71a7          1  cb2f3c5d71a7.png  ben_colour_cb2f3c5d71a7.png\n",
      "2931  cb39761f0712          0  cb39761f0712.png  ben_colour_cb39761f0712.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Samples Per Class')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data frame of current competition\n",
    "df_2019 = pd.read_csv(f\"../aptos2019/train.csv\") \n",
    "df_2019.processed = df_2019.id_code.apply(lambda x: pre_process + x + \".png\")\n",
    "df_2019[\"original\"]  = df_2019.id_code.apply(lambda x: x + \".png\")\n",
    "df_2019[\"processed\"] = df_2019.original.apply(lambda x: pre_process + x)\n",
    "train_2019, valid_2019 = train_test_split(df_2019, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 2019 data frame \n",
    "df_2015 = pd.read_csv(f\"../aptos2015/trainLabels.csv\") \n",
    "df_2015[\"original\"]  = df_2015.image.apply(lambda x: x + \".jpeg\")\n",
    "df_2015[\"processed\"] = df_2015.original.apply(lambda x: pre_process + x)\n",
    "df_2015[\"diagnosis\"] = df_2015.level\n",
    "train_2015, valid_2015 = train_test_split(df_2015, test_size=0.2, shuffle=False)\n",
    "\n",
    "valid_2019['diagnosis'].value_counts().plot(kind='bar')\n",
    "print(valid_2019[:3])\n",
    "plt.title('Samples Per Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_convert(y_val):\n",
    "    y_val = y_val.astype(int).sum(axis=1)\n",
    "    return y_val\n",
    "\n",
    "def get_train_valid_df(year=\"2019\", even_distrib=True):\n",
    "    \n",
    "    # shuffle data so each time different samples are dropped\n",
    "    if (year == \"2019\"):\n",
    "        train = train_2019.sample(frac=1)\n",
    "        valid = valid_2019.sample(frac=1)\n",
    "    elif (year == \"2015\"):\n",
    "        train = train_2015.sample(frac=1)\n",
    "        valid = valid_2015.sample(frac=1)\n",
    "    \n",
    "    # remap from classes to smoothed version of the classes\n",
    "    train[\"labels\"] = train.diagnosis.apply(lambda x: [i for i in range(x)])\n",
    "    valid[\"labels\"] = valid.diagnosis.apply(lambda x: [i for i in range(x)])\n",
    "\n",
    "    # drop classes \n",
    "    if even_distrib:\n",
    "        min_train = min(train['diagnosis'].value_counts())\n",
    "        min_valid = min(valid['diagnosis'].value_counts())\n",
    "\n",
    "        for diagnosis in range(5):\n",
    "            indexes_valid = valid[valid['diagnosis'] == diagnosis].index\n",
    "            indexes_train = train[train['diagnosis'] == diagnosis].index\n",
    "            \n",
    "            frac_drop_train = indexes_train.size * (1 - min_train/indexes_train.size)\n",
    "            frac_drop_valid = indexes_valid.size * (1 - min_valid/indexes_valid.size)\n",
    "            \n",
    "            train.drop(indexes_train[:int(frac_drop_train)], inplace=True)\n",
    "            valid.drop(indexes_valid[:int(frac_drop_valid)], inplace=True)\n",
    "\n",
    "    # shuffle it for even distribution\n",
    "    train = train.sample(frac=0.8)\n",
    "    valid = valid.sample(frac=0.8)\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "# df_sample, _ = get_train_valid_df(year=\"2019\")\n",
    "# print(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display some data\n",
    "# def display_data():\n",
    "#     df_example, _ = get_train_valid_df(year=\"2019\")\n",
    "\n",
    "#     # Display some random images from Data Set with class categories.\n",
    "#     figure=plt.figure(figsize=(22,20))\n",
    "#     for target_class in range(5):\n",
    "#         for i, file_name in enumerate(df_example[df_example.diagnosis == target_class].original.sample(2)):\n",
    "            \n",
    "#             # open the file\n",
    "#             o_img = cv2.imread(f\"../aptos2019/train_images/{file_name}\")\n",
    "#             p_img = cv2.imread(f\"../aptos2019/train_images/{pre_process}{file_name}\")\n",
    "\n",
    "#             o_rgb = cv2.cvtColor(o_img, cv2.COLOR_BGR2RGB)\n",
    "#             p_rgb = cv2.cvtColor(p_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "#             ax = figure.add_subplot(5,4, int(target_class)*4+2*i+1)\n",
    "#             plt.imshow(o_rgb)\n",
    "#             ax.set_title(target_class)\n",
    "            \n",
    "#             ax = figure.add_subplot(5,4, int(target_class)*4+2*i+2)\n",
    "#             plt.imshow(p_rgb)\n",
    "#             ax.set_title(target_class)\n",
    "            \n",
    "# display_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data generators, which put their data into a random crop generator, which is then fed into\n",
    "# the network during training\n",
    "\n",
    "def dataGenerator(jitter=0.1):\n",
    "    datagen = image.ImageDataGenerator(rescale=1./255,\n",
    "                                       horizontal_flip=True and (jitter > 0.01), \n",
    "                                       vertical_flip=True and (jitter > 0.01),\n",
    "                                       rotation_range=int(800*jitter),\n",
    "                                       brightness_range=[1-jitter, 1+jitter],\n",
    "                                       channel_shift_range=int(30*jitter),\n",
    "                                       zoom_range=[(1-jitter), (1+jitter/2)],\n",
    "                                       fill_mode=\"reflect\",\n",
    "                                      )\n",
    "    return datagen\n",
    "\n",
    "def datagen_with_flow(datagen, dataframe, directory):\n",
    "    return datagen.flow_from_dataframe(dataframe=dataframe, directory=directory,\n",
    "                                       x_col=\"processed\", \n",
    "                                       y_col='labels', \n",
    "                                       class_mode=\"categorical\", \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       target_size=(IMG_DIM, IMG_DIM),\n",
    "                                       shuffle=False,\n",
    "                                      )\n",
    "\n",
    "def generator(jitter=0.1, year=\"2019\", even_distrib=True):\n",
    "    \n",
    "    train, valid = get_train_valid_df(year=year, even_distrib=even_distrib) \n",
    "    datagen = dataGenerator(jitter)\n",
    "    \n",
    "    train_gen = datagen_with_flow(datagen, train, f\"../aptos{year}/train_images/\")\n",
    "    valid_gen = datagen_with_flow(datagen, valid, f\"../aptos{year}/train_images/\")\n",
    "    \n",
    "    return train_gen, valid_gen\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample_gen, valid_sample_gen = generator(jitter=0.5)\n",
    "\n",
    "# # Display some data generation\n",
    "# figure=plt.figure(figsize=(22,20))\n",
    "# for batch in valid_sample_gen:\n",
    "#     for j in range(16):\n",
    "#         ax = figure.add_subplot(4,4, j+1)\n",
    "#         batch[0][j] = np.clip(batch[0][j], 0, 1)\n",
    "#         plt.imshow(batch[0][j])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        numBatches = 20\n",
    "        y_pred     = []\n",
    "        y_val      = []\n",
    "        for x, y in self.generator:\n",
    "            predictions = model.predict(x) \n",
    "            y_pred.extend(label_convert(predictions > 0.5))\n",
    "            y_val.extend(label_convert(y))\n",
    "            \n",
    "            numBatches -= 1\n",
    "            if numBatches <= 0:\n",
    "                break\n",
    "            \n",
    "        val_kappa = cohen_kappa_score(y_val, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(val_kappa)\n",
    "        \n",
    "        print(confusion_matrix(y_val, y_pred), val_kappa)\n",
    "            \n",
    "        if val_kappa == max(self.val_kappas) and val_kappa > 0.85:\n",
    "            gc.collect()\n",
    "            print(\"Max of this run, saving model.\")\n",
    "            model.save(f\"{pre_process}-{val_kappa:.4f}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 03:33:12.976200 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0817 03:33:13.003152 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0817 03:33:13.011469 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0817 03:33:13.040630 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0817 03:33:13.041404 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0817 03:33:13.981774 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0817 03:33:14.070655 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0817 03:33:15.422746 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0817 03:33:47.967903 140367202748160 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(DenseNet121(weights='../DenseNet-BC-121-32-no-top.h5', \n",
    "#     model.add(DenseNet121(weights=None, \n",
    "                          include_top=False, \n",
    "                          input_shape=(IMG_DIM, IMG_DIM, CHANNEL_SIZE)))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 03:33:48.026004 140367202748160 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0817 03:33:48.037008 140367202748160 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ----------------------------------- 2019 0.5 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    True -   -   -   -   -   -   -   -   -\n",
      "Found 632 validated image filenames belonging to 4 classes.\n",
      "Found 140 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "19/19 [==============================] - 71s 4s/step - loss: 0.8625 - acc: 0.4827 - val_loss: 0.7669 - val_acc: 0.4902\n",
      "[[ 8 29 45 26  0]\n",
      " [ 8 16 36 28  8]\n",
      " [ 0 10 33 52 13]\n",
      " [ 5 11 44 45 15]\n",
      " [ 5 19 39 45 20]] 0.19938229804944918\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 20s 1s/step - loss: 0.8180 - acc: 0.5152 - val_loss: 0.7126 - val_acc: 0.5486\n",
      "[[10 39 45 14  0]\n",
      " [ 9 17 41 27  2]\n",
      " [ 0 12 35 44 17]\n",
      " [ 5 10 44 42 19]\n",
      " [ 3 18 32 48 27]] 0.33147023086269745\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 17s 912ms/step - loss: 0.7548 - acc: 0.5580 - val_loss: 0.6962 - val_acc: 0.5694\n",
      "[[17 42 39  8  2]\n",
      " [ 6 21 38 26  5]\n",
      " [ 0  8 37 39 24]\n",
      " [ 1 14 33 44 28]\n",
      " [ 1 14 38 48 27]] 0.3939820902999561\n",
      "           -   -   -   -   -   -   -   -    False -   -   -   -   -   -   -   -   -\n",
      "Found 2343 validated image filenames belonging to 4 classes.\n",
      "Found 586 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "73/73 [==============================] - 76s 1s/step - loss: 0.6253 - acc: 0.6605 - val_loss: 0.5231 - val_acc: 0.7504\n",
      "[[158 105  36  23   6]\n",
      " [  8  13   9  10  10]\n",
      " [  4   8  26  69  41]\n",
      " [  1   3   4  13  10]\n",
      " [  3   2  11  20  25]] 0.5818189281234494\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 71s 967ms/step - loss: 0.4643 - acc: 0.7828 - val_loss: 0.4515 - val_acc: 0.7987\n",
      "[[216  76  22  11   3]\n",
      " [ 11   9  15  12   3]\n",
      " [  3   8  41  56  40]\n",
      " [  0   2   5   8  16]\n",
      " [  2   1  13  13  32]] 0.7045524760140527\n",
      "Epoch 3/3\n",
      "73/73 [==============================] - 70s 954ms/step - loss: 0.3664 - acc: 0.8459 - val_loss: 0.3502 - val_acc: 0.8660\n",
      "[[269  40  14   3   2]\n",
      " [ 12  11  17   8   2]\n",
      " [  4  11  67  43  23]\n",
      " [  1   1   8  11  10]\n",
      " [  2   1  12  24  22]] 0.7752887672664442\n",
      "           ----------------------------------- 2019 0.4 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    True -   -   -   -   -   -   -   -   -\n",
      "Found 632 validated image filenames belonging to 4 classes.\n",
      "Found 140 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5350 - acc: 0.7566 - val_loss: 0.4149 - val_acc: 0.7988\n",
      "[[110   8   2   0   0]\n",
      " [ 21  35  36   4   8]\n",
      " [  3   7  43  41  18]\n",
      " [  1   7  20  30  50]\n",
      " [  2  17  30  34  33]] 0.6818357945494468\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 17s 915ms/step - loss: 0.4951 - acc: 0.7633 - val_loss: 0.4333 - val_acc: 0.7755\n",
      "[[111   7   2   0   0]\n",
      " [ 22  34  32  13   3]\n",
      " [  4  10  48  31  19]\n",
      " [  4   3  26  40  35]\n",
      " [  4  13  35  33  31]] 0.6724033816425121\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 17s 919ms/step - loss: 0.4475 - acc: 0.7944 - val_loss: 0.3999 - val_acc: 0.8079\n",
      "[[113   7   0   0   0]\n",
      " [ 28  37  29   5   5]\n",
      " [  3  12  48  38  11]\n",
      " [  2   7  27  45  27]\n",
      " [  3   8  47  28  30]] 0.6990318012597476\n",
      "           -   -   -   -   -   -   -   -    False -   -   -   -   -   -   -   -   -\n",
      "Found 2343 validated image filenames belonging to 4 classes.\n",
      "Found 586 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "73/73 [==============================] - 76s 1s/step - loss: 0.3301 - acc: 0.8641 - val_loss: 0.2614 - val_acc: 0.9076\n",
      "[[290  26   4   1   0]\n",
      " [ 10  14  21   3   1]\n",
      " [  1   3 104  45   5]\n",
      " [  0   0  10  14   8]\n",
      " [  1   1  21  23  12]] 0.8483283266955738\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 70s 956ms/step - loss: 0.2824 - acc: 0.8881 - val_loss: 0.2203 - val_acc: 0.9246\n",
      "[[301  14   6   0   0]\n",
      " [  7  20  19   3   0]\n",
      " [  1  11 125  19   2]\n",
      " [  0   1  12  15   4]\n",
      " [  1   2  34  14   7]] 0.8348249191953903\n",
      "Epoch 3/3\n",
      "73/73 [==============================] - 70s 954ms/step - loss: 0.2630 - acc: 0.8970 - val_loss: 0.2040 - val_acc: 0.9305\n",
      "[[306  11   4   0   0]\n",
      " [  7  18  22   2   0]\n",
      " [  3  11 113  30   1]\n",
      " [  0   0  13  14   5]\n",
      " [  0   5  26  22   5]] 0.8439447330736485\n",
      "           ----------------------------------- 2019 0.3 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    True -   -   -   -   -   -   -   -   -\n",
      "Found 632 validated image filenames belonging to 4 classes.\n",
      "Found 140 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "19/19 [==============================] - 69s 4s/step - loss: 0.4674 - acc: 0.7874 - val_loss: 0.3489 - val_acc: 0.8359\n",
      "[[107   1   4   0   0]\n",
      " [ 25  42  37   8   0]\n",
      " [  9  11  83  17   0]\n",
      " [  0   3  37  63   9]\n",
      " [  3   4  46  38  13]] 0.7205474987779046\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 17s 917ms/step - loss: 0.4257 - acc: 0.8021 - val_loss: 0.3570 - val_acc: 0.8380\n",
      "[[108   0   4   0   0]\n",
      " [ 32  44  30   6   0]\n",
      " [ 11  13  78  18   0]\n",
      " [  0   4  41  55  12]\n",
      " [  2  15  46  30  11]] 0.6807582083472745\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 18s 924ms/step - loss: 0.4048 - acc: 0.8069 - val_loss: 0.3848 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-07.\n",
      "[[108   1   3   0   0]\n",
      " [ 35  49  25   3   0]\n",
      " [ 13  19  72  16   0]\n",
      " [  1   6  42  58   5]\n",
      " [  4  17  42  30  11]] 0.6630097437748106\n",
      "           -   -   -   -   -   -   -   -    False -   -   -   -   -   -   -   -   -\n",
      "Found 2343 validated image filenames belonging to 4 classes.\n",
      "Found 586 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "73/73 [==============================] - 76s 1s/step - loss: 0.2736 - acc: 0.8896 - val_loss: 0.2034 - val_acc: 0.9219\n",
      "[[320  10   3   0   0]\n",
      " [ 10  19  23   2   1]\n",
      " [  5  10 103  34   3]\n",
      " [  0   0   6  16   1]\n",
      " [  0   5  19  21   7]] 0.846663959940452\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 69s 949ms/step - loss: 0.2616 - acc: 0.8934 - val_loss: 0.2099 - val_acc: 0.9138\n",
      "[[310  17   6   0   0]\n",
      " [  6  20  24   5   0]\n",
      " [  4   7 100  40   4]\n",
      " [  0   0   6  12   5]\n",
      " [  1   3  15  23  10]] 0.8470620099924066\n",
      "Epoch 3/3\n",
      "73/73 [==============================] - 70s 953ms/step - loss: 0.2628 - acc: 0.8931 - val_loss: 0.2069 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-08.\n",
      "[[309  19   5   0   0]\n",
      " [  6  17  29   3   0]\n",
      " [  2   5 100  44   4]\n",
      " [  0   0   6  15   2]\n",
      " [  0   4  18  23   7]] 0.8491991746094134\n",
      "           ----------------------------------- 2019 0.05 -----------------------------------\n",
      "           -   -   -   -   -   -   -   -    True -   -   -   -   -   -   -   -   -\n",
      "Found 632 validated image filenames belonging to 4 classes.\n",
      "Found 140 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "19/19 [==============================] - 72s 4s/step - loss: 0.4370 - acc: 0.8076 - val_loss: 0.3293 - val_acc: 0.8477\n",
      "[[122   1   1   0   0]\n",
      " [ 19  29  54   6   0]\n",
      " [  7  18  69  30   0]\n",
      " [  0   2  34  55   5]\n",
      " [  1   7  45  38  17]] 0.735481909394953\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 17s 913ms/step - loss: 0.4229 - acc: 0.8134 - val_loss: 0.3528 - val_acc: 0.8356\n",
      "[[124   0   0   0   0]\n",
      " [ 22  40  40   6   0]\n",
      " [  8  16  76  24   0]\n",
      " [  0   2  35  52   7]\n",
      " [  3   8  46  36  15]] 0.7256904220477396\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 17s 920ms/step - loss: 0.4178 - acc: 0.8221 - val_loss: 0.3691 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.500000053056283e-07.\n",
      "[[123   1   0   0   0]\n",
      " [ 23  44  36   4   1]\n",
      " [  9  21  70  24   0]\n",
      " [  0   4  32  53   7]\n",
      " [  4   9  51  29  15]] 0.7021572819809773\n",
      "           -   -   -   -   -   -   -   -    False -   -   -   -   -   -   -   -   -\n",
      "Found 2343 validated image filenames belonging to 4 classes.\n",
      "Found 586 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "73/73 [==============================] - 77s 1s/step - loss: 0.2719 - acc: 0.8882 - val_loss: 0.1929 - val_acc: 0.9232\n",
      "[[318  13   2   0   0]\n",
      " [  6  19  22   1   0]\n",
      " [  3  11 109  35   2]\n",
      " [  0   1   9  14   1]\n",
      " [  0   2  16  28   6]] 0.8745809593962809\n",
      "Max of this run, saving model.\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 70s 959ms/step - loss: 0.2636 - acc: 0.8894 - val_loss: 0.2026 - val_acc: 0.9201\n",
      "[[315  18   0   0   0]\n",
      " [  9  11  25   3   0]\n",
      " [  2  10 110  35   3]\n",
      " [  0   0   7  13   5]\n",
      " [  0   3  18  22   9]] 0.8682549369004321\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 70s 963ms/step - loss: 0.2662 - acc: 0.8900 - val_loss: 0.2060 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.500000053056283e-08.\n",
      "[[306  22   4   1   0]\n",
      " [  6  17  24   1   0]\n",
      " [  2   7 115  35   1]\n",
      " [  0   0   7  14   4]\n",
      " [  0   3  18  24   7]] 0.8618194331539601\n"
     ]
    }
   ],
   "source": [
    "for year in [ \"2019\"]:\n",
    "    for jitter in [0.5, 0.4, 0.3, 0.05]:\n",
    "    \n",
    "        model.compile(optimizer=Adam(lr=0.00003*jitter), loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "        print(\"           -----------------------------------\", year, \n",
    "              jitter, \"-----------------------------------\")\n",
    "\n",
    "        for even_distrib in [True, False]:\n",
    "\n",
    "            print(\"           -   -   -   -   -   -   -   -   \", \n",
    "                  even_distrib, \"-   -   -   -   -   -   -   -   -\")\n",
    "            \n",
    "            # these need to be global for the kappa callback\n",
    "            train_generator, valid_generator = generator(jitter=jitter, year=year, even_distrib=even_distrib)\n",
    "\n",
    "            # Call backs during training            \n",
    "            kappa_callbacks = Metrics(valid_generator)\n",
    "            reduce_lr  = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, \n",
    "                                           min_lr=1e-8, mode='auto', verbose=1)\n",
    "\n",
    "            # train the model for 12 epochs\n",
    "            history = model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=train_generator.n  // train_generator.batch_size,\n",
    "                                          validation_data=valid_generator,\n",
    "                                          validation_steps=valid_generator.n // valid_generator.batch_size,\n",
    "                                          epochs=3, workers=4, verbose=1,\n",
    "                                          callbacks=[reduce_lr, kappa_callbacks],\n",
    "                                         )\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{pre_process}-201[59]-run.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the average of 5 randomised jitters to a non-jittered val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_prediction_process(year=\"2019\"):\n",
    "\n",
    "#     _, valid_df = get_train_valid_df(year=year, even_distrib=False)\n",
    "    \n",
    "#     y_val  = valid_df.diagnosis.astype(int)\n",
    "\n",
    "#     # with jitter\n",
    "#     num = 7\n",
    "#     prediction_lists = np.zeros((valid_df.index.size, num, 5))\n",
    "#     for i in range(num):\n",
    "#         datagen = datagen_with_flow(dataGenerator(0.03), valid_df, f\"../aptos{year}/train_images/\")\n",
    "#         prediction_lists[:, i] = model.predict_generator(generator=datagen, steps=len(datagen), workers=4, verbose=1)\n",
    "\n",
    "#     predictions = np.median(prediction_lists, axis=1)\n",
    "#     y_pred = label_convert(predictions > 0.5)\n",
    "    \n",
    "#     print(\"With jitter: \", cohen_kappa_score(y_val, y_pred, weights='quadratic'))\n",
    "#     print(confusion_matrix(y_val, y_pred))\n",
    "          \n",
    "#     # no jitter\n",
    "#     datagen = datagen_with_flow(dataGenerator(0), valid_df, f\"../aptos{year}/train_images/\")\n",
    "#     predictions = model.predict_generator(generator=datagen, steps=len(datagen), workers=4, verbose=1)\n",
    "#     y_pred = label_convert(predictions > 0.5)\n",
    "    \n",
    "#     print(\"With no jitter: \", cohen_kappa_score(y_val, y_pred, weights='quadratic'))\n",
    "#     print(confusion_matrix(y_val, y_pred))\n",
    "            \n",
    "    \n",
    "# compare_prediction_process(\"2019\")\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

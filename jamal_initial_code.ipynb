{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2 \n",
    "import PIL \n",
    "import gc\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from tensorflow import set_random_seed\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.activations import softmax, relu\n",
    "from keras.optimizers import Adam, rmsprop, RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'aptos2019-blindness-detection/train.csv' does not exist: b'aptos2019-blindness-detection/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d37384434da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# take in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aptos2019-blindness-detection/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aptos2019-blindness-detection/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'aptos2019-blindness-detection/train.csv' does not exist: b'aptos2019-blindness-detection/train.csv'"
     ]
    }
   ],
   "source": [
    "SEED = 7\n",
    "np.random.seed(SEED) \n",
    "set_random_seed(SEED)\n",
    "\n",
    "# take in data\n",
    "df_train = pd.read_csv(\"aptos2019-blindness-detection/train.csv\") \n",
    "df_test  = pd.read_csv(\"aptos2019-blindness-detection/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIM      = 299 #512\n",
    "BATCH_SIZE   = 4\n",
    "CHANNEL_SIZE = 3\n",
    "NUM_EPOCHS   = 12\n",
    "FREEZE_LAYERS = 2  # freeze the first this many layers for training\n",
    "NUM_CLASSES = df_train['diagnosis'].nunique()\n",
    "\n",
    "CLASSS={0:\"No DR\", 1:\"Mild\", 2:\"Moderate\", 3:\"Severe\", 4:\"Proliferative DR\"}\n",
    "\n",
    "df_train.columns, NUM_CLASSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['diagnosis'].value_counts().plot(kind='bar');\n",
    "plt.title('Samples Per Class');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(df_train.id_code, df_train.diagnosis, test_size=0.2, random_state =SEED, stratify=df_train.diagnosis)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample image\n",
    "imgPath = f\"aptos2019-blindness-detection/train_images/cd54d022e37d.png\"\n",
    "img = cv2.imread(imgPath)\n",
    "print(img.shape)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some random images from Data Set with class categories.\n",
    "figure=plt.figure(figsize=(20,12))\n",
    "for target_class in (y_train.unique()):\n",
    "#     print(CLASSS[target_class],target_class)\n",
    "    for i, (idx, row ) in enumerate(df_train.loc[df_train.diagnosis==target_class].sample(3, random_state=SEED).iterrows()):\n",
    "        ax = figure.add_subplot(5,5, target_class*5+i+1)\n",
    "        imagefile = f\"aptos2019-blindness-detection/train_images/{row['id_code']}.png\" \n",
    "        img = cv2.imread(imagefile)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img,(IMG_DIM,IMG_DIM))\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(CLASSS[target_class])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def squareUp(y, ycc):\n",
    "    \n",
    "    thresh = 5\n",
    "    \n",
    "    # remove any black regions, where black is y under the threshold\n",
    "    rowMaxes = y.max(axis=1)\n",
    "    top = 0\n",
    "    while rowMaxes[top] < thresh:\n",
    "        top += 1\n",
    "    bottom = len(rowMaxes) - 1\n",
    "    while rowMaxes[bottom] < thresh:\n",
    "        bottom -= 1\n",
    "        \n",
    "    middleRow = y[int((bottom-top)/2)]\n",
    "    left = 0\n",
    "    while middleRow[left] < thresh:\n",
    "        left += 1\n",
    "    right = len(middleRow) - 1\n",
    "    while middleRow[right] < thresh:\n",
    "        right -= 1\n",
    "        \n",
    "    height = bottom - top\n",
    "    width  = right - left\n",
    "        \n",
    "    if height < 100 or width < 100:\n",
    "        print(\"Error: squareUp: bottom:\", bottom, \"top:\", top)\n",
    "        print(\"Error: squareUp: right:\", right, \"left:\", left)\n",
    "        return ycc\n",
    "    \n",
    "    # Make a blank canvas, of the size of a square of the larger of the above\n",
    "    length = max(height, width)\n",
    "    blank_image = np.full((length, length, 3), 128, np.uint8)\n",
    "    blank_image[:,:,0] = 0 # black in ycc is 0 128 128\n",
    "    \n",
    "    ycc = ycc[top:bottom, left:right]\n",
    "    v_offset  = int((length - height)/2)\n",
    "    h_offset = int((length - width)/2)\n",
    "    blank_image[v_offset:v_offset+height, h_offset:h_offset+width] = ycc\n",
    "    \n",
    "    return blank_image\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some random images from Data Set with class categories. showig Gray image removing other channel and adding lighting to image.\n",
    "def colourfulEyes(img):\n",
    "    \n",
    "    #assumes RGB\n",
    "        \n",
    "    # convert to y, cr, cb so that we can modify the image based on just the y (brightness)\n",
    "    ycc = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    y, cr, cb = cv2.split(ycc)\n",
    "\n",
    "    # remove and add black to square the image\n",
    "    # ycc = squareUp(y, ycc)\n",
    "    # y, cr, cb = cv2.split(ycc)\n",
    "\n",
    "    # perform bens algorithm on the y component\n",
    "    y = cv2.addWeighted(y, 3, cv2.GaussianBlur(y, (0,0), 60),-3, 128)\n",
    "\n",
    "    # merge the ycc back together, and recolor it\n",
    "    ycc_modified = cv2.merge((y, cr, cb))\n",
    "    rgb_modified = cv2.cvtColor(ycc_modified, cv2.COLOR_YCrCb2RGB)\n",
    "    \n",
    "    return rgb_modified\n",
    "        \n",
    "figure=plt.figure(figsize=(25,16))\n",
    "for target_class in (y_train.unique()):\n",
    "#     print(CLASSS[target_class],target_class)\n",
    "    for i, (idx, row ) in enumerate(df_train.loc[df_train.diagnosis==target_class].sample(4, random_state=345).iterrows()):\n",
    "\n",
    "        # open the file\n",
    "        imagefile=f\"aptos2019/train_images/{row['id_code']}.png\" \n",
    "        img = cv2.imread(imagefile)\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        lefuckded = colourfulEyes(rgb)\n",
    "        \n",
    "        #         img = cv2.resize(img,(IMG_DIM,IMG_DIM))\n",
    "        ax = figure.add_subplot(5,5, target_class*5+i+1)\n",
    "        plt.imshow(rgb)\n",
    "        ax = figure.add_subplot(5,5, target_class*5+i+2)\n",
    "        plt.imshow(lefuckded)\n",
    "        break\n",
    "        \n",
    "#     break\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1] \n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1) \n",
    "    img=img[y:(y+dy), x:(x+dx), :] \n",
    "    return img\n",
    "    \n",
    "\"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"  \n",
    "def crop_generator(batches, crop_length):    \n",
    "    while True:\n",
    "        batch_x, batch_y=next(batches)\n",
    "        batch_crops=np.zeros((batch_x.shape[0],crop_length,3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[0]=random_crop(batch_x[i],(crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "df_train.id_code=df_train.id_code.apply(lambda x: x+\".png\")\n",
    "df_test.id_code=df_test.id_code.apply(lambda x: x+\".png\")\n",
    "df_train['diagnosis'] = df_train['diagnosis'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen=image.ImageDataGenerator(rescale=1./255,\n",
    "#                                        validation_split=0.15,\n",
    "#                                        horizontal_flip=True,\n",
    "#                                        vertical_flip=True,\n",
    "#                                        shear_range=0.1,\n",
    "#                                        zoom_range=0.1)\n",
    "#                                        featurewise_center=True, \n",
    "#                                        featurewise_std_normalization=True\n",
    "train_datagen=image.ImageDataGenerator(rescale=1./255,validation_split=0.2,horizontal_flip=True, preprocessing_function=colourfulEyes)\n",
    "\n",
    "# valid_datagen=image.ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=train_datagen.flow_from_dataframe(dataframe=df_train,\n",
    "                                                  directory=\"aptos2019-blindness-detection/train_images/\",\n",
    "                                                  x_col=\"id_code\",\n",
    "                                                  y_col=\"diagnosis\",\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  target_size=(IMG_DIM, IMG_DIM),\n",
    "                                                  subset='training', \n",
    "                                                  shaffle=False,\n",
    "                                                  seed=SEED,\n",
    "                                                 )\n",
    "valid_generator=train_datagen.flow_from_dataframe(dataframe=df_train,\n",
    "                                                  directory=\"aptos2019-blindness-detection/train_images/\",\n",
    "                                                  x_col=\"id_code\",\n",
    "                                                  y_col=\"diagnosis\",\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  target_size=(IMG_DIM, IMG_DIM),\n",
    "                                                  subset='validation',\n",
    "                                                  shaffle=False,\n",
    "                                                  seed=SEED\n",
    "                                                 )\n",
    "del x_train \n",
    "# # del x_test\n",
    "del y_train\n",
    "# del y_test\n",
    "gc.collect()\n",
    "#  color_mode= \"grayscale\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n",
    "# Reducing the Learning Rate if result is not improving. \n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6,  mode='auto', verbose=1)\n",
    "\n",
    "NUB_TRAIN_STEPS=train_generator.n//train_generator.batch_size\n",
    "NUB_VALID_STEPS=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "NUB_TRAIN_STEPS, NUB_VALID_STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet(img_dim, CHANNEL, n_class):\n",
    "    input_tensor=Input(shape=(img_dim, img_dim, CHANNEL))\n",
    "  \n",
    "    base_model = ResNet50(weights=None, include_top=False, input_tensor=input_tensor)\n",
    "    base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation=relu)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(246, activation=relu)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Dense(n_class,activation='softmax', name=\"Output_Layer\")(x)\n",
    "    model_resnet = Model(input_tensor, output_layer)\n",
    "    \n",
    "    return model_resnet\n",
    "\n",
    "model_resnet=create_resnet(IMG_DIM, CHANNEL_SIZE, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in model_resnet.layers:\n",
    "    layers.trainable=True\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = rmsprop(lr=lr, decay=0.1)\n",
    "model_resnet.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy,  metrics=['accuracy'])\n",
    "# model.summary()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_resnet.fit_generator(generator=train_generator,\n",
    "                           steps_per_epoch=NUB_TRAIN_STEPS,\n",
    "                           validation_data=valid_generator,\n",
    "                           validation_steps=NUB_VALID_STEPS,\n",
    "                           epochs=NUM_EPOCHS,\n",
    "#                            shuffle=True,  \n",
    "                           callbacks=[early_stop, reduce_lr],\n",
    "                           verbose=2)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "(eval_loss, eval_accuracy)=tqdm(model_resnet.evaluate_generator(generator=valid_generator,steps=NUB_VALID_STEPS, pickle_safe=False))\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=image.ImageDataGenerator(rescale=1./255,validation_split=0.2,horizontal_flip=True)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(dataframe=df_test,\n",
    "                                                directory = \"aptos2019-blindness-detection/test_images/\",\n",
    "                                                x_col=\"id_code\",\n",
    "                                                target_size=(IMG_DIM, IMG_DIM),\n",
    "                                                batch_size=1,\n",
    "                                                shuffle=False, \n",
    "                                                class_mode=None, \n",
    "                                                seed=SEED)\n",
    "# del df_test\n",
    "print(df_test.shape[0])\n",
    "# del train_datagen\n",
    "# del traabsin_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_steps = 10\n",
    "preds_tta=[]\n",
    "for i in tqdm(range(tta_steps)):\n",
    "    test_generator.reset()\n",
    "    preds = model_resnet.predict_generator(generator=test_generator,steps =ceil(df_test.shape[0]))\n",
    "#     print('Before ', preds.shape)\n",
    "    preds_tta.append(preds)\n",
    "#     print(i,  len(preds_tta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.mean(preds_tta, axis=0)\n",
    "predicted_class_indices = np.argmax(final_pred, axis=1)\n",
    "len(predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_generator.filenames.apply(lambda x: x[-4])\n",
    "results=pd.DataFrame({\"id_code\":test_generator.filenames, \"diagnosis\":predicted_class_indices})  \n",
    "results.id_code=results.id_code.apply(lambda x: x[:-4])# results.head()\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
